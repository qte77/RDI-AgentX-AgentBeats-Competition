{
  "project": "AgentBeats-Benchmarking-Enhancements",
  "source": "docs/PRD.md",
  "generated": "2026-01-16T02:20:00Z",
  "description": "TDD-enforced implementation with TEST/IMPL pairs: A2A protocol fix, real LLM integration, latency metrics",
  "stories": [
    {
      "id": "STORY-001-TEST",
      "title": "Write A2A SDK messenger tests",
      "description": "Write tests defining messenger contract for A2A SDK integration (test file creation = passing)",
      "acceptance": [
        "tests/test_messenger.py updated with A2A SDK tests",
        "Tests define expected behavior for ClientFactory.connect()",
        "Tests define expected behavior for create_text_message_object()",
        "Tests mock async iteration over send_message() events",
        "Tests verify client caching per agent URL",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_messenger.py"
      ],
      "passes": true,
      "completed_at": "2026-01-16T02:29:42Z"
    },
    {
      "id": "STORY-001-IMPL",
      "title": "Refactor messenger to use A2A SDK",
      "description": "Implement messenger.py to pass A2A SDK tests",
      "acceptance": [
        "src/agentbeats/messenger.py imports ClientFactory, create_text_message_object from a2a.client",
        "talk_to_agent() uses await ClientFactory.connect(agent_url)",
        "Messages created via create_text_message_object(content=message)",
        "Response extracted from TaskState.completed events",
        "Client caching per agent URL implemented",
        "uv run pytest tests/test_messenger.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/messenger.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-002-TEST",
      "title": "Write TraceData task_id field tests",
      "description": "Write tests defining TraceData task_id field requirement",
      "acceptance": [
        "tests/test_messenger.py updated with task_id tests",
        "Tests verify TraceData model has task_id: str | None = None field",
        "Tests verify task.id is captured in TraceData",
        "Tests verify existing fields preserved (timestamp, agent_url, message, response, status_code, error)",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_messenger.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-002-IMPL",
      "title": "Add task_id field to TraceData",
      "description": "Implement task_id tracking in TraceData model",
      "acceptance": [
        "TraceData model has new field: task_id: str | None = None",
        "talk_to_agent() captures task.id from A2A Task object",
        "Existing fields preserved",
        "uv run pytest tests/test_messenger.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/messenger.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-003-TEST",
      "title": "Write Executor A2A cleanup tests",
      "description": "Write tests defining Executor cleanup contract for A2A clients",
      "acceptance": [
        "tests/test_executor.py updated with cleanup tests",
        "Tests verify Executor.execute() calls await messenger.close()",
        "Tests verify Messenger.close() cleans up cached clients",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_executor.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-003-IMPL",
      "title": "Implement Executor A2A cleanup",
      "description": "Implement executor cleanup to pass tests",
      "acceptance": [
        "Executor.execute() calls await messenger.close() after collecting traces",
        "Messenger has close() method to cleanup cached clients",
        "uv run pytest tests/test_messenger.py tests/test_executor.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/executor.py",
        "src/agentbeats/messenger.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-004",
      "title": "Add OpenAI dependency",
      "description": "Add openai SDK to project dependencies for LLM integration",
      "acceptance": [
        "pyproject.toml contains openai>=1.0 in dependencies",
        "uv sync succeeds",
        "uv run python -c 'import openai' succeeds"
      ],
      "files": [
        "pyproject.toml"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-005-TEST",
      "title": "Write LLM client config tests",
      "description": "Write tests defining LLM client configuration contract",
      "acceptance": [
        "tests/test_llm_judge.py updated with config tests",
        "Tests verify LLMJudge reads AGENTBEATS_LLM_API_KEY",
        "Tests verify LLMJudge reads AGENTBEATS_LLM_BASE_URL (default: https://api.openai.com/v1)",
        "Tests verify LLMJudge reads AGENTBEATS_LLM_MODEL (default: gpt-4o-mini)",
        "Tests verify OpenAI-compatible endpoint support",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-005-IMPL",
      "title": "Implement LLM client config",
      "description": "Implement LLM client configuration to pass tests",
      "acceptance": [
        "LLMJudge reads environment variables: AGENTBEATS_LLM_API_KEY, AGENTBEATS_LLM_BASE_URL, AGENTBEATS_LLM_MODEL",
        "Default base URL is https://api.openai.com/v1",
        "Default model is gpt-4o-mini",
        "Client supports any OpenAI-compatible endpoint",
        "uv run pytest tests/test_llm_judge.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/evals/llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-006-TEST",
      "title": "Write LLM prompt tests",
      "description": "Write tests defining LLM evaluation prompt contract",
      "acceptance": [
        "tests/test_llm_judge.py updated with prompt tests",
        "Tests verify prompt serializes TraceData list",
        "Tests verify prompt asks for: overall_score (0-1), reasoning, coordination_quality, strengths, weaknesses",
        "Tests verify prompt requests JSON-formatted response matching LLMJudgment schema",
        "Tests verify prompt includes evaluation criteria",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-006-IMPL",
      "title": "Implement LLM prompt",
      "description": "Implement LLM evaluation prompt to pass tests",
      "acceptance": [
        "Prompt serializes TraceData list into readable format",
        "Prompt asks for: overall_score (0-1), reasoning, coordination_quality, strengths, weaknesses",
        "Prompt requests JSON-formatted response matching LLMJudgment schema",
        "Prompt includes clear evaluation criteria for coordination quality",
        "uv run pytest tests/test_llm_judge.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/evals/llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-007-TEST",
      "title": "Write LLM API fallback tests",
      "description": "Write tests defining LLM API calls with fallback contract",
      "acceptance": [
        "tests/test_llm_judge.py updated with API fallback tests",
        "Tests mock openai.ChatCompletion.create() (or equivalent async call)",
        "Tests verify LLM response parsing into LLMJudgment",
        "Tests verify fallback behavior when API fails",
        "Tests verify fallback behavior when API key not set",
        "Tests verify warning logged when using fallback (not error)",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-007-IMPL",
      "title": "Implement LLM API with fallback",
      "description": "Implement LLM API calls with fallback to pass tests",
      "acceptance": [
        "If AGENTBEATS_LLM_API_KEY is set, use LLM API",
        "If LLM call fails or key not set, fall back to rule-based logic",
        "Log warning when using fallback (not error)",
        "Parse LLM JSON response into LLMJudgment object",
        "Handle API errors gracefully (timeout, invalid JSON, etc.)",
        "uv run pytest tests/test_llm_judge.py passes",
        "make type_check passes"
      ],
      "files": [
        "src/agentbeats/evals/llm_judge.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-008-TEST",
      "title": "Write latency evaluator tests",
      "description": "Write tests defining latency metrics evaluator contract",
      "acceptance": [
        "tests/test_latency.py exists with focused tests",
        "Tests define expected behavior for LatencyEvaluator.evaluate(traces)",
        "Tests verify timestamp parsing from TraceData",
        "Tests verify percentile calculations: avg, p50, p95, p99",
        "Tests verify slowest agent URL identification",
        "Test file is syntactically valid Python"
      ],
      "files": [
        "tests/test_latency.py"
      ],
      "passes": false,
      "completed_at": null
    },
    {
      "id": "STORY-008-IMPL",
      "title": "Implement latency evaluator",
      "description": "Implement latency metrics evaluator to pass tests",
      "acceptance": [
        "src/agentbeats/evals/latency.py exists",
        "Follows existing evaluator pattern (like GraphEvaluator, LLMJudge)",
        "Parses timestamps from TraceData",
        "Computes: avg response time, p50, p95, p99",
        "Identifies slowest agent URL",
        "Executor._evaluate_latency() method added (follows tier pattern)",
        "Results included as tier1_latency in Executor response",
        "uv run pytest tests/test_latency.py passes",
        "make type_check and make test_agent pass"
      ],
      "files": [
        "src/agentbeats/evals/latency.py",
        "src/agentbeats/executor.py"
      ],
      "passes": false,
      "completed_at": null
    }
  ]
}
